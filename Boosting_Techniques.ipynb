{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques  Assignment"
      ],
      "metadata": {
        "id": "zINfoNvAG0Xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Boosting is an **ensemble learning technique** in machine learning that combines multiple **weak learners** to create a **stronger predictive model**. A weak learner is a model that performs slightly better than random guessing. Boosting sequentially trains these weak learners, with each new model focusing on the mistakes made by the previous ones.\n",
        "\n",
        "Here’s a step-by-step explanation of how it works and improves weak learners:\n",
        "\n",
        "1. **Start with a weak learner**:\n",
        "   Typically, a simple model like a shallow decision tree (also called a “stump”) is trained on the data. This model may have limited accuracy but captures some patterns.\n",
        "\n",
        "2. **Identify errors**:\n",
        "   After the first model makes predictions, Boosting identifies the data points that were misclassified or predicted poorly.\n",
        "\n",
        "3. **Increase focus on difficult cases**:\n",
        "   The next weak learner is trained to **pay more attention to the errors** of the previous model. In practice, this is done by giving higher weights to misclassified examples, so the new model focuses on correcting them.\n",
        "\n",
        "4. **Sequential combination**:\n",
        "   This process is repeated for many weak learners. Each learner tries to improve on the errors of the ensemble so far.\n",
        "\n",
        "5. **Weighted voting or prediction**:\n",
        "   In the end, all the weak learners are combined into a single strong model. Usually, the predictions are weighted based on each learner’s accuracy, so better learners contribute more.\n",
        "\n",
        "### How Boosting Improves Weak Learners\n",
        "\n",
        "* **Error correction**: Each new learner compensates for the weaknesses of the previous ones.\n",
        "* **Focus on hard cases**: Difficult-to-predict data points get more attention over iterations.\n",
        "* **Reduced bias**: Combining many weak learners often produces a model with much lower bias than individual learners.\n",
        "\n",
        "### Popular Boosting Algorithms\n",
        "\n",
        "* **AdaBoost**: Adjusts weights of misclassified examples and combines learners via weighted voting.\n",
        "* **Gradient Boosting**: Trains each new learner to predict the residual errors of the current ensemble.\n",
        "* **XGBoost / LightGBM / CatBoost**: Optimized, faster, and more regularized versions of gradient boosting.\n",
        "\n",
        "---\n",
        "\n",
        "# Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "The main difference between **AdaBoost** and **Gradient Boosting** lies in **how each new model is trained to correct the errors of the previous models**. Both are boosting methods, but their mechanisms differ:\n",
        "\n",
        "### **1. AdaBoost (Adaptive Boosting)**\n",
        "\n",
        "* **Error-focused weighting**:\n",
        "  AdaBoost adjusts the **weights of the training examples** after each iteration. Misclassified examples get higher weights so that the next weak learner focuses more on them.\n",
        "\n",
        "* **Training approach**:\n",
        "  Each weak learner is trained **independently** on the weighted dataset. The algorithm does not explicitly minimize a loss function; it emphasizes correcting past mistakes.\n",
        "\n",
        "* **Combination of learners**:\n",
        "  Each learner’s vote is weighted based on its accuracy: better-performing learners contribute more to the final prediction.\n",
        "\n",
        "* **Intuition**:\n",
        "  “Focus on the mistakes of the previous learner by giving them more importance.”\n",
        "\n",
        "### **2. Gradient Boosting**\n",
        "\n",
        "* **Error-focused prediction (residuals)**:\n",
        "  Gradient Boosting trains each new learner to **predict the residual errors** (the difference between actual values and predictions) of the current ensemble.\n",
        "\n",
        "* **Training approach**:\n",
        "  Each learner is trained **sequentially to minimize a loss function** (like MSE for regression or log-loss for classification) using gradient descent. This is why it’s called “gradient” boosting.\n",
        "\n",
        "* **Combination of learners**:\n",
        "  Learners are added to the ensemble with a learning rate that scales their contribution to reduce overfitting.\n",
        "\n",
        "* **Intuition**:\n",
        "  “Learn the gradient of the loss function to correct the errors of the ensemble step by step.”\n",
        "\n",
        "### **Key Differences in Training**\n",
        "\n",
        "| Feature                | AdaBoost                         | Gradient Boosting                                                     |\n",
        "| ---------------------- | -------------------------------- | --------------------------------------------------------------------- |\n",
        "| How errors are handled | Reweights misclassified examples | Learns to predict residuals (gradients)                               |\n",
        "| Focus                  | Misclassified samples            | Loss minimization                                                     |\n",
        "| Learner weighting      | Based on accuracy                | Learning rate controls contribution                                   |\n",
        "| Algorithm style        | Sequential, adaptive weighting   | Sequential, gradient descent optimization                             |\n",
        "| Flexibility            | Mostly for classification        | Works for both classification and regression; flexible loss functions |\n",
        "\n",
        "---\n",
        "\n",
        "#Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "In **XGBoost**, regularization plays a key role in **controlling model complexity** and **preventing overfitting**, which is especially important because boosting builds many trees sequentially that can easily overfit the training data.\n",
        "\n",
        "Here’s how it works:\n",
        "\n",
        "### **1. Regularization in XGBoost**\n",
        "\n",
        "XGBoost includes **explicit regularization terms** in its objective function:\n",
        "\n",
        "$$\n",
        "\\text{Obj} = \\text{Loss}(y, \\hat{y}) + \\Omega(f)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\text{Loss}(y, \\hat{y})$ is the prediction error (e.g., mean squared error for regression).\n",
        "* $\\Omega(f)$ is the regularization term for the tree model $f$.\n",
        "\n",
        "The regularization term is defined as:\n",
        "\n",
        "$$\n",
        "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $T$ = number of leaves in the tree\n",
        "* $w_j$ = score (weight) of leaf $j$\n",
        "* $\\gamma$ = penalty for adding an extra leaf (controls tree size)\n",
        "* $\\lambda$ = L2 regularization on leaf weights (shrinks weights)\n",
        "\n",
        "### **2. How Regularization Helps**\n",
        "\n",
        "1. **Prevents overfitting**\n",
        "\n",
        "   * Penalizes overly complex trees with too many leaves.\n",
        "   * Limits large leaf weights that could overreact to training data noise.\n",
        "\n",
        "2. **Controls tree complexity**\n",
        "\n",
        "   * $\\gamma$ discourages unnecessary splits.\n",
        "   * $\\lambda$ encourages smaller weights, making predictions smoother.\n",
        "\n",
        "3. **Improves generalization**\n",
        "\n",
        "   * By balancing fit (loss minimization) with complexity, XGBoost produces models that perform better on unseen data.\n",
        "\n",
        "4. **Supports tuning flexibility**\n",
        "\n",
        "   * Users can adjust `gamma` and `lambda` to find the optimal trade-off between bias and variance.\n",
        "\n",
        "---\n",
        "\n",
        "#Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "CatBoost is considered efficient for handling categorical data because it **natively processes categorical features** without requiring extensive preprocessing like one-hot encoding or label encoding, which are commonly used in other gradient boosting frameworks. Here’s why it works so well:\n",
        "\n",
        "### **1. Native Categorical Handling**\n",
        "\n",
        "* CatBoost can directly accept categorical features as input.\n",
        "* It uses a method called **“ordered target statistics”** (or target encoding) to convert categories into numerical representations based on the target variable.\n",
        "* This avoids creating a huge number of dummy variables, which can slow down training and increase memory usage.\n",
        "\n",
        "### **2. Ordered Target Statistics**\n",
        "\n",
        "* Instead of using the global mean of the target for encoding (which can cause **target leakage**), CatBoost calculates the mean **in an ordered, iterative manner**.\n",
        "* For example, when encoding a row, it only uses the previous rows in the dataset to compute the statistic, ensuring that the model does not “peek” at the target for the current row.\n",
        "\n",
        "### **3. Efficient Handling of High Cardinality**\n",
        "\n",
        "* Categories with many unique values (like ZIP codes or product IDs) are handled efficiently because CatBoost doesn’t explode the feature space like one-hot encoding would.\n",
        "* It reduces memory usage and improves training speed without sacrificing accuracy.\n",
        "\n",
        "### **4. Built-in Feature Combinations**\n",
        "\n",
        "* CatBoost can automatically generate **combinations of categorical features** that are likely to improve predictive power, without manually engineering interaction features.\n",
        "\n",
        "### **5. GPU Optimization**\n",
        "\n",
        "* CatBoost is optimized to train on both CPU and GPU efficiently, including for datasets with large categorical features.\n",
        "\n",
        "---\n",
        "\n",
        "# Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Datasets:\n",
        "\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "#Answer:\n",
        "\n",
        "Here’s a practical perspective on **real-world applications where boosting is preferred over bagging**, illustrated with **examples using the datasets you mentioned**.\n",
        "\n",
        "## **1. Classification Task: Breast Cancer Diagnosis**\n",
        "\n",
        "**Dataset:** `sklearn.datasets.load_breast_cancer()`\n",
        "\n",
        "* **Problem type:** Binary classification (malignant vs benign tumors)\n",
        "* **Why boosting is preferred:**\n",
        "\n",
        "  * Boosting sequentially focuses on misclassified tumor samples, improving sensitivity to the harder-to-classify cases (e.g., tumors with ambiguous features).\n",
        "  * Reduces bias, which is crucial in medical diagnostics where misclassification can be critical.\n",
        "\n",
        "**Example boosting models:**\n",
        "\n",
        "* **AdaBoostClassifier**: Uses weak decision stumps to focus on difficult-to-classify tumors.\n",
        "* **GradientBoostingClassifier / XGBoost / CatBoost**: Optimizes loss and handles feature interactions automatically.\n",
        "\n",
        "**Real-world analog:**\n",
        "\n",
        "* Predicting cancer malignancy from patient histopathology or imaging features, where each patient case may have subtle variations.\n",
        "\n",
        "## **2. Regression Task: California Housing Prices**\n",
        "\n",
        "**Dataset:** `sklearn.datasets.fetch_california_housing()`\n",
        "\n",
        "* **Problem type:** Regression (predict median house value from features like location, income, rooms, etc.)\n",
        "* **Why boosting is preferred:**\n",
        "\n",
        "  * Boosting reduces bias and sequentially corrects underpredicted or overpredicted house values.\n",
        "  * Captures complex nonlinear relationships between features (e.g., geography, income, population density) that simple bagging may underfit.\n",
        "\n",
        "**Example boosting models:**\n",
        "\n",
        "* **GradientBoostingRegressor**: Minimizes residuals iteratively.\n",
        "* **XGBoost / LightGBM / CatBoost**: Efficient on large datasets with mixed feature types and can handle high cardinality features.\n",
        "\n",
        "**Real-world analog:**\n",
        "\n",
        "* Real estate price prediction or property valuation platforms, where small improvements in prediction accuracy directly impact decision-making.\n",
        "\n",
        "### **Why Boosting Beats Bagging in These Cases**\n",
        "\n",
        "| Aspect             | Breast Cancer Classification                   | California Housing Regression                      |\n",
        "| ------------------ | ---------------------------------------------- | -------------------------------------------------- |\n",
        "| Goal               | Detect subtle malignant cases                  | Predict complex price patterns                     |\n",
        "| Challenge          | Hard-to-classify tumors                        | Nonlinear feature interactions                     |\n",
        "| Boosting advantage | Focuses on misclassified samples; reduces bias | Reduces bias; fits residual errors iteratively     |\n",
        "| Bagging limitation | Might not correct bias in subtle cases         | Averaging many trees may underfit complex patterns |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BSJ9J5TiG2_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "KWTXhG4vJTtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost classifier\n",
        "adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obwiD4h5JXEe",
        "outputId": "c48ab860-026c-400b-eeb7-3f1e80ddf0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7:  Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "Zk1Qc_MCJoLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=200,      # Number of boosting stages\n",
        "    learning_rate=0.1,     # Step size shrinkage\n",
        "    max_depth=3,           # Maximum depth of individual trees\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Regressor R-squared score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU1XIRPNJfWI",
        "outputId": "7fc6dcb4-55bc-487e-94f4-ce0349bf4843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score: 0.8004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "_xuUQlLXKEY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create XGBoost classifier\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "# Define learning rates to test\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV accuracy:\", round(grid_search.best_score_, 4))\n",
        "\n",
        "# Test accuracy\n",
        "y_pred = grid_search.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", round(test_accuracy, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0Cj798LLDy",
        "outputId": "28aabc4d-f6e2-4ef6-bd19-d43ed9cd6cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'learning_rate': 0.2}\n",
            "Best CV accuracy: 0.967\n",
            "Test accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "fzqW-kFkMAIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost scikit-learn seaborn matplotlib\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Ensure target is integer (for CatBoost compatibility)\n",
        "y = y.astype(int)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoostClassifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,       # number of trees\n",
        "    learning_rate=0.1,\n",
        "    depth=3,\n",
        "    verbose=0             # suppress logging\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = y_pred.astype(int)  # Ensure prediction is int for confusion_matrix\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "P65-lrCoGUxG",
        "outputId": "f657fe3b-3799-4891-d0c1-bf25214437a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model"
      ],
      "metadata": {
        "id": "79N443KjM6LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer:\n",
        "\n",
        "## **1. Data Preprocessing**\n",
        "\n",
        "### **a. Handling Missing Values**\n",
        "\n",
        "* **Numeric features:**\n",
        "\n",
        "  * Impute missing values using **median** (robust to outliers) or use **iterative imputation**.\n",
        "* **Categorical features:**\n",
        "\n",
        "  * Impute with **mode** or a special category like `\"Unknown\"`.\n",
        "* Some boosting libraries like **CatBoost** can handle missing values natively, but explicit imputation is often safer.\n",
        "\n",
        "### **b. Handling Categorical Features**\n",
        "\n",
        "* **AdaBoost / XGBoost:** Requires numeric encoding:\n",
        "\n",
        "  * Use **one-hot encoding** for low-cardinality categories.\n",
        "  * Use **target encoding** or **CatBoost-style encoding** for high-cardinality features.\n",
        "* **CatBoost:** Can directly handle categorical features without encoding, making it more efficient.\n",
        "\n",
        "### **c. Feature Scaling**\n",
        "\n",
        "* Most tree-based boosting models **don’t require scaling**, so you can skip normalization or standardization unless using hybrid models.\n",
        "\n",
        "### **d. Handling Imbalanced Data**\n",
        "\n",
        "* **Techniques:**\n",
        "\n",
        "  * Adjust **class weights** in the loss function (`scale_pos_weight` in XGBoost, `class_weights` in CatBoost).\n",
        "  * Oversample minority class using **SMOTE** or undersample majority class.\n",
        "* Boosting naturally emphasizes misclassified examples, which helps with imbalanced datasets.\n",
        "\n",
        "## **2. Choice of Boosting Algorithm**\n",
        "\n",
        "| Algorithm | Why Choose It                                                                                 |\n",
        "| --------- | --------------------------------------------------------------------------------------------- |\n",
        "| AdaBoost  | Simple, interpretable, good if dataset is small and mostly numeric                            |\n",
        "| XGBoost   | Efficient, flexible, supports missing values, handles large datasets                          |\n",
        "| CatBoost  | Best for datasets with **categorical features** and missing values, less preprocessing needed |\n",
        "\n",
        "**Recommendation:** **CatBoost** is preferred here due to categorical features and missing values. XGBoost is also strong if categorical variables are already encoded.\n",
        "\n",
        "## **3. Hyperparameter Tuning Strategy**\n",
        "\n",
        "* **Parameters to tune:**\n",
        "\n",
        "  * `learning_rate` (step size for boosting)\n",
        "  * `n_estimators` (number of boosting rounds)\n",
        "  * `max_depth` (depth of individual trees)\n",
        "  * `subsample` / `colsample_bytree` (fraction of samples/features per tree)\n",
        "  * `l2_leaf_reg` (CatBoost regularization)\n",
        "* **Tuning methods:**\n",
        "\n",
        "  * **RandomizedSearchCV** for efficiency on large datasets.\n",
        "  * **GridSearchCV** for smaller parameter spaces.\n",
        "  * Optionally, **Bayesian optimization** (e.g., Optuna) for more efficient tuning.\n",
        "\n",
        "## **4. Evaluation Metrics**\n",
        "\n",
        "* **Due to imbalanced dataset:**\n",
        "\n",
        "  * **ROC-AUC score:** Measures how well the model separates classes, insensitive to imbalance.\n",
        "  * **Precision, Recall, F1-score:** Especially important for the minority class (defaults).\n",
        "  * **Confusion matrix:** Understand false positives vs false negatives.\n",
        "* **Why not accuracy alone:**\n",
        "\n",
        "  * A model predicting all “non-default” could have high accuracy but fail to detect real defaults.\n",
        "\n",
        "## **5. Business Impact**\n",
        "\n",
        "* **Early identification of risky borrowers:** Allows the company to mitigate losses by adjusting loan terms, collateral, or interest rates.\n",
        "* **Optimized credit approval:** Reduces defaults while keeping profitable customers.\n",
        "* **Targeted interventions:** Customers at risk of default can receive reminders or financial support programs.\n",
        "* **Regulatory compliance:** Accurate and interpretable models help meet lending regulations and risk reporting standards.\n",
        "\n",
        "### **6. Optional: Pipeline Workflow Summary**\n",
        "\n",
        "1. **Load data → Handle missing values → Encode categorical variables** (or use CatBoost directly).\n",
        "2. **Split data into train/test sets → Handle imbalance (weights or SMOTE).**\n",
        "3. **Train boosting model (CatBoost/XGBoost) → Tune hyperparameters via CV.**\n",
        "4. **Evaluate using ROC-AUC, F1-score, confusion matrix.**\n",
        "5. **Deploy model → Monitor predictions and retrain periodically.**\n"
      ],
      "metadata": {
        "id": "zyiZ6MBxNSYh"
      }
    }
  ]
}