{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        "\n",
        "Ans. A Decision Tree is a supervised machine learning algorithm used for classification tasks. It works by splitting the dataset into subsets based on the value of input features. Each internal node in the tree represents a decision on a feature, each branch represents an outcome of that decision, and each leaf node represents a class label.\n",
        "\n",
        "The model uses rules like \"if-else\" conditions to make decisions and classify data step-by-step down the tree.\n",
        "\n",
        "**Example:**\n",
        "To predict if a customer will buy a product:\n",
        "\n",
        "* If Age > 30\n",
        "\n",
        "  * If Income > 50k → Predict: Buy\n",
        "  * Else → Predict: Don’t Buy\n",
        "* Else → Predict: Don’t Buy"
      ],
      "metadata": {
        "id": "eYHn2PQ_hUfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "Ans. **Gini Impurity** and **Entropy** are impurity measures used to decide how a decision tree splits data at each node.\n",
        "\n",
        "* **Gini Impurity** measures the probability of incorrectly classifying a randomly chosen element.\n",
        "\n",
        "  * Formula: *Gini = 1 − Σ (pᵢ)²*, where *pᵢ* is the probability of class *i*.\n",
        "\n",
        "* **Entropy** measures the amount of disorder or uncertainty in the data.\n",
        "\n",
        "  * Formula: *Entropy = −Σ (pᵢ × log₂(pᵢ))*\n",
        "\n",
        "**Impact on Splits:**\n",
        "\n",
        "* Both measures help find the best feature to split on by evaluating how \"pure\" the resulting subsets will be.\n",
        "* Lower impurity means better splits.\n",
        "* The decision tree selects the feature that gives the **highest information gain** (for entropy) or **largest reduction in Gini**."
      ],
      "metadata": {
        "id": "ArvB_9SWAOME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans. Pre-Pruning (Early Stopping): Pre-pruning involves stopping the growth of the decision tree during the training phase. This is done by setting constraints such as maximum tree depth, minimum number of samples required to split a node, or a threshold on impurity reduction.\n",
        "\n",
        "Advantage: It reduces training time and prevents the model from overfitting by avoiding unnecessary complexity from the start.\n",
        "\n",
        "Post-Pruning (Pruning After Full Growth): Post-pruning allows the tree to grow fully and then removes branches that do not contribute significantly to the model's performance. This is usually done using a validation set or cross-validation.\n",
        "\n",
        "Advantage: It improves the model’s generalization by simplifying a complex tree, which can lead to better performance on unseen data."
      ],
      "metadata": {
        "id": "2JAR0kurLV6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "**Ans.** Information Gain is a key concept used in building Decision Trees. It measures how much “information” a feature provides about the target class by quantifying the reduction in entropy (uncertainty or impurity) when a dataset is split based on that feature.\n",
        "\n",
        "The formula for Information Gain (IG) when splitting a dataset $D$ on attribute $A$ is:\n",
        "\n",
        "$$\n",
        "IG(D, A) = Entropy(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} \\cdot Entropy(D_v)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Entropy(D)$ is the entropy of the entire dataset.\n",
        "* $D_v$ is the subset of data for which attribute $A$ has value $v$.\n",
        "* $|D_v| / |D|$ is the proportion of data in $D_v$.\n",
        "* $Entropy(D_v)$ is the entropy of subset $D_v$.\n",
        "\n",
        "**Importance of Information Gain:**\n",
        "\n",
        "* It helps select the best attribute to split the data at each node in the decision tree.\n",
        "* A higher Information Gain indicates that the feature better separates the data into distinct classes.\n",
        "* It leads to more accurate and efficient decision trees by reducing impurity and improving classification performance.\n",
        "\n",
        "**Example:**\n",
        "In a dataset used to predict whether a person will play tennis, if the feature \"Outlook\" results in the highest Information Gain, then the decision tree will split on \"Outlook\" first, as it provides the most useful information for classification."
      ],
      "metadata": {
        "id": "TXP3OcXk8yVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "**Ans:** **Real-World Applications of Decision Trees:**\n",
        "\n",
        "1. **Medical Diagnosis:**\n",
        "   Used to diagnose diseases based on symptoms, test results, and patient history.\n",
        "\n",
        "2. **Loan Approval and Credit Scoring:**\n",
        "   Used by banks to decide loan approvals based on factors like credit score, income, and employment status.\n",
        "\n",
        "3. **Marketing and Customer Segmentation:**\n",
        "   Helps businesses identify target customer groups and personalize marketing strategies.\n",
        "\n",
        "4. **Fraud Detection:**\n",
        "   Detects unusual patterns in transactions that may indicate fraudulent activity.\n",
        "\n",
        "5. **Manufacturing and Quality Control:**\n",
        "   Used to identify causes of defects and improve product quality.\n",
        "\n",
        "**Main Advantages of Decision Trees:**\n",
        "\n",
        "* Easy to understand and interpret.\n",
        "* Can handle both numerical and categorical data.\n",
        "* No need for feature scaling or normalization.\n",
        "* Can model non-linear relationships.\n",
        "* Handles missing values effectively.\n",
        "\n",
        "**Main Limitations of Decision Trees:**\n",
        "\n",
        "* Prone to overfitting, especially with deep trees.\n",
        "* Sensitive to small changes in data (instability).\n",
        "* Can be biased toward features with many categories.\n",
        "* Single decision trees often have lower accuracy compared to ensemble methods."
      ],
      "metadata": {
        "id": "7Zdn0Lv2_ptZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances'''\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIUgqh4gA3S8",
        "outputId": "047302a9-1ade-4c79-c8a5-6a3f04fe7934"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "'''\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "model_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_limited.fit(X_train, y_train)\n",
        "y_pred_limited = model_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully-grown Decision Tree (no depth limit)\n",
        "model_full = DecisionTreeClassifier(random_state=42)\n",
        "model_full.fit(X_train, y_train)\n",
        "y_pred_full = model_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print both accuracies\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2F0hODcBG0x",
        "outputId": "8e9b9cf6-f00e-4b58-f3ba-33b8e85e8f62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "'''\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZo3mk00BfQ7",
        "outputId": "4e7fa2a6-f4c6-4ed6-b77f-633aa949668e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy'''\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to tune\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters from GridSearch\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYDzrRtAE9fh",
        "outputId": "178de9f1-639c-4e93-a8fd-d82eee5e5d8e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10:\n",
        "Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "Answer:\n",
        "1. Handle Missing Values\n",
        "Understand the missingness: First, analyze how data is missing. Is it random, or does it follow a pattern? This affects how you handle it.\n",
        "\n",
        "Imputation:\n",
        "\n",
        "For numerical features, you could fill missing values with mean, median, or use more advanced methods like K-Nearest Neighbors imputation.\n",
        "For categorical features, you might fill missing with the mode (most frequent value) or create a special category like \"Unknown\".\n",
        "If missing values are too prevalent or critical, consider dropping those features or samples carefully.\n",
        "Why it matters: Models can’t handle missing data directly, so cleaning this up ensures your model sees complete, reliable inputs.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "Identify categorical variables: This could be patient gender, blood type, or any non-numeric info.\n",
        "\n",
        "Encoding methods:\n",
        "\n",
        "For nominal categories without order (e.g., blood type), use One-Hot Encoding.\n",
        "For ordinal categories (e.g., disease severity: mild, moderate, severe), use Label Encoding or map them to meaningful numeric scales.\n",
        "Why it matters: Machine learning models, including Decision Trees, require numeric input, so encoding transforms your data into a digestible form.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "Split the data: Use an 80-20 or 70-30 split between training and test sets, or use cross-validation to ensure your results generalize.\n",
        "\n",
        "Initialize the model: Start with a default Decision Tree classifier.\n",
        "\n",
        "Train on processed data: Fit the model on your training data.\n",
        "\n",
        "Why Decision Trees: They handle mixed data types well, are interpretable (important in healthcare), and can capture nonlinear patterns.\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "Key hyperparameters to tune:\n",
        "\n",
        "max_depth: controls tree complexity, balancing underfitting and overfitting.\n",
        "min_samples_split and min_samples_leaf: control how many samples needed to split or be a leaf node, affecting generalization.\n",
        "max_features: number of features to consider at each split.\n",
        "Use GridSearchCV or RandomizedSearchCV: Explore combinations systematically with cross-validation to find the sweet spot.\n",
        "\n",
        "Why tuning matters: Proper tuning prevents overfitting or underfitting, improving the model’s predictive power on unseen data.\n",
        "\n",
        "5. Evaluate Performance\n",
        "Metrics:\n",
        "\n",
        "For classification, consider Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "In healthcare, Recall (sensitivity) is often critical — you want to catch as many patients with the disease as possible, even at the cost of some false positives.\n",
        "Validation: Use a separate test set or cross-validation to ensure your model performs reliably.\n",
        "\n",
        "Interpretability: Use feature importance and decision tree visualization to explain model decisions to clinicians and stakeholders.\n",
        "\n",
        "Business Value of This Model\n",
        "Early detection: Predicting disease early means patients can receive timely treatment, improving outcomes and reducing healthcare costs.\n",
        "\n",
        "Resource optimization: Helps healthcare providers prioritize high-risk patients for screening or intervention, making better use of limited resources.\n",
        "\n",
        "Personalized care: Tailors monitoring and care plans based on individual risk, improving patient satisfaction and effectiveness.\n",
        "\n",
        "Data-driven decisions: Provides actionable insights backed by data, enabling the company to develop better products, policies, or outreach programs.\n",
        "\n",
        "Trust and transparency: Decision Trees’ interpretability supports building trust with clinicians, regulators, and patients, crucial in healthcare."
      ],
      "metadata": {
        "id": "pfkWNR5hFsdc"
      }
    }
  ]
}