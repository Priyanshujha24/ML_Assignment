{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning assignment"
      ],
      "metadata": {
        "id": "or5uWm5wXCMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Ensemble Learning in machine learning Explain the key idea behind it.**\n",
        "\n",
        "**Ans.** Ensemble Learning is a machine learning paradigm where multiple models are trained to solve the same problem and their predictions are combined to obtain better performance than any individual model. It is based on the concept that a group of weak models can come together to form a stronger model.\n",
        "\n",
        "There are several popular ensemble learning techniques, each with different strategies for combining models:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "How it works: Multiple models are trained independently on different random subsets of the training data (created through bootstrapping). The final prediction is made by averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "Goal: Reduce variance and prevent overfitting.\n",
        "\n",
        "Popular algorithm: Random Forest\n",
        "\n",
        "Example: In Random Forest, multiple decision trees are trained on random subsets of data and features. Their predictions are combined to produce a more accurate and stable model.\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "How it works: Models are trained sequentially. Each new model focuses on correcting the errors made by the previous model.\n",
        "\n",
        "Goal: Reduce bias and improve accuracy.\n",
        "\n",
        "Popular algorithms: AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
        "\n",
        "Example: In AdaBoost, the first model is trained on the data. The second model is trained more heavily on the data points that were misclassified by the first model, and so on.\n",
        "\n",
        "3. Stacking (Stacked Generalization)\n",
        "\n",
        "How it works: Different types of models are trained on the same dataset. Then, another model (called a meta-learner) is trained to combine their outputs.\n",
        "\n",
        "Goal: Leverage the strengths of multiple models.\n",
        "Example: Combine a decision tree, a support vector machine, and a neural network, then use a logistic regression model to learn the best way to combine their predictions."
      ],
      "metadata": {
        "id": "bzV1LvTZW-VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**Ans.** Difference Between Bagging and Boosting\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**\n",
        "   Training style: Models are trained in parallel on different random subsets of the training data (using sampling with replacement).\n",
        "   Goal: Reduce variance (helps prevent overfitting).\n",
        "   How it works: Each model is trained independently, and their predictions are combined by voting (for classification) or averaging (for regression). Since models are trained on different subsets, the ensemble becomes more stable and less sensitive to noise.\n",
        "   Example: Random Forest, which combines multiple decision trees trained on bootstrapped data samples.\n",
        "\n",
        "2. **Boosting**\n",
        "   Training style: Models are trained sequentially. Each new model focuses on the mistakes made by the previous models.\n",
        "   Goal: Reduce bias (helps improve accuracy on hard-to-predict cases).\n",
        "   How it works: Initially, all data points are given equal weight. After each model is trained, the weights of misclassified instances are increased so that the next model pays more attention to them. The final prediction is made by combining the outputs of all models, typically using a weighted majority vote or weighted sum.\n",
        "   Examples: AdaBoost, Gradient Boosting, XGBoost — all of which are commonly used in classification and regression problems."
      ],
      "metadata": {
        "id": "Nbg2KnWcePtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is Bootstrap Sampling and What Role Does It Play in Bagging Methods Like Random Forest?**\n",
        "\n",
        "**Ans.** Bootstrap sampling is a statistical technique that involves randomly selecting samples **with replacement** from a dataset to create multiple new training datasets. Each of these samples is the same size as the original dataset but may contain duplicate entries due to the replacement process, while some original data points may be left out.\n",
        "\n",
        "In the context of **Bagging (Bootstrap Aggregating)** methods like **Random Forest**, bootstrap sampling plays a crucial role. In Bagging, multiple models (usually decision trees) are trained in parallel, each on a different bootstrapped sample of the original data. This means every model sees a slightly different version of the dataset, leading to **model diversity**.\n",
        "\n",
        "This diversity helps reduce **variance** and makes the overall ensemble model more **robust and stable**. When all the individual models make their predictions, the final output is determined by **majority voting** (for classification) or **averaging** (for regression), which smooths out the individual errors and improves overall accuracy.\n",
        "\n",
        "Therefore, bootstrap sampling is essential in Bagging techniques like Random Forest because it ensures that each model learns different patterns and contributes uniquely to the final prediction."
      ],
      "metadata": {
        "id": "yU807wYspXIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are Out-of-Bag (OOB) Samples and How is OOB Score Used to Evaluate Ensemble Models?**\n",
        "\n",
        "**Ans.** Out-of-Bag (OOB) samples are the data points **not included** in a particular bootstrap sample during the training of an ensemble model like **Random Forest**. Since bootstrap sampling is done **with replacement**, about **one-third** of the original data is typically left out (not selected) for any given model. These unused data points are called OOB samples for that specific model.\n",
        "\n",
        "In ensemble methods like Random Forest, **OOB samples are used as a built-in validation set** to evaluate the performance of the model without needing a separate test set or cross-validation. Each tree in the forest is tested on its corresponding OOB samples (the data it didn’t see during training). The predictions made on these samples are collected across all trees, and the aggregated result is used to calculate the **OOB score**.\n",
        "\n",
        "The **OOB score** is the accuracy (or error rate) of the model based on these OOB predictions. It provides an **unbiased estimate** of the model’s performance on unseen data and is especially useful when the dataset is small, as it avoids the need to split the data into training and testing sets.\n",
        "\n",
        "In summary, OOB samples act as an internal validation set, and the OOB score helps evaluate the ensemble model’s generalization performance efficiently."
      ],
      "metadata": {
        "id": "HbvEP_maqQ1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Compare Feature Importance Analysis in a Single Decision Tree vs. a Random Forest**\n",
        "\n",
        "**Ans.** Feature importance refers to techniques used to identify which input features have the most influence on a model’s predictions.\n",
        "\n",
        "In a **single Decision Tree**, feature importance is calculated based on how much each feature reduces an impurity measure (like Gini impurity or entropy) at each split. The more a feature is used to split the data and the greater the improvement in purity, the higher its importance score. However, this approach is **sensitive to noise and overfitting**, especially if the tree is deep or trained on a small dataset.\n",
        "\n",
        "In contrast, a **Random Forest**, which is an ensemble of many decision trees trained on different bootstrapped samples and random feature subsets, provides a **more reliable and stable** estimate of feature importance. It calculates feature importance by averaging the impurity reduction (or other importance scores) of each feature **across all trees** in the forest. Because it aggregates over many models, Random Forests reduce the effect of overfitting and are **less biased** toward features with more levels or categories.\n",
        "\n",
        "In summary, while both models use impurity-based calculations for feature importance, a **single Decision Tree may give unstable or biased results**, whereas a **Random Forest provides more robust, consistent, and generalizable feature importance scores**."
      ],
      "metadata": {
        "id": "t2y_h329rPaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cJFQk-vAdIUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fde0c1-aa0e-4976-8ff4-24b033a6d5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to pair features with their importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # updated argument\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_accuracy)\n",
        "print(\"Accuracy of Bagging Classifier:\", bagging_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avn6PqtWsCOk",
        "outputId": "438f701a-4c47-4632-8984-7a57615b586b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [2, 4, 6, None]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Make predictions using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy on Test Set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP3Q3vRPsjUn",
        "outputId": "4d8c044a-5793-4ffe-fa40-5ded5c22291e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 150}\n",
            "Final Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)'''\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with Decision Trees\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),  # updated argument\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", bagging_mse)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", rf_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31BfBKP2svJI",
        "outputId": "f4ecbb86-7f82-4a9c-f52d-5c4b5ed366a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2568358813508342\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.**\n",
        "\n",
        "**Ans.** Step by Step approach:\n",
        "\n",
        "Step 1: Choose Between Bagging or Boosting\n",
        "Factors to consider:\n",
        "\n",
        "Data size: Large datasets can benefit from Bagging, as it trains many models in parallel efficiently.\n",
        "\n",
        "Bias vs Variance:\n",
        "\n",
        "Bagging reduces variance → useful if individual models (like Decision Trees) tend to overfit.\n",
        "Boosting reduces bias → useful if single models underfit and can learn sequentially from mistakes.\n",
        "Noise sensitivity: Boosting can be sensitive to noisy data, which can lead to overfitting in financial datasets.\n",
        "\n",
        "Decision:\n",
        "\n",
        "Start with Bagging (e.g., Random Forest) to get a robust baseline.\n",
        "If underfitting is detected, try Boosting (e.g., XGBoost, LightGBM) to improve predictive power.\n",
        "Step 2: Handle Overfitting\n",
        "Techniques:\n",
        "\n",
        "Limit tree depth (max_depth) and minimum samples per leaf to prevent overly complex trees.\n",
        "Use ensemble averaging: Bagging naturally reduces overfitting by averaging predictions.\n",
        "Regularization in Boosting: Parameters like learning_rate and n_estimators in XGBoost control overfitting.\n",
        "Cross-validation: Monitor performance on validation sets to detect overfitting early.\n",
        "Step 3: Select Base Models\n",
        "Common base models for ensemble techniques:\n",
        "\n",
        "Decision Trees → widely used for Bagging and Boosting.\n",
        "Logistic Regression → can be used in stacking ensembles for interpretability.\n",
        "Other weak learners like small neural networks or SVMs if boosting/staking is used.\n",
        "Financial context tip:\n",
        "\n",
        "Decision Trees are preferred due to interpretability, which is important for regulatory compliance in financial institutions.\n",
        "Step 4: Evaluate Performance Using Cross-Validation\n",
        "Split data into k folds (e.g., 5 or 10).\n",
        "\n",
        "Train ensemble models on k-1 folds and validate on the remaining fold.\n",
        "\n",
        "Metrics to monitor:\n",
        "\n",
        "ROC-AUC → captures model ability to distinguish defaulters vs non-defaulters.\n",
        "Precision/Recall → especially if default cases are rare (imbalanced data).\n",
        "F1-Score → balances precision and recall.\n",
        "Average metrics across folds for a reliable performance estimate.\n",
        "\n",
        "Optional: Use Stratified K-Fold to ensure class proportions are maintained in each fold.\n",
        "\n",
        "Step 5: Justify How Ensemble Learning Improves Decision-Making\n",
        "Improved Accuracy:\n",
        "\n",
        "Combining multiple models reduces variance and bias, leading to more reliable predictions of defaults.\n",
        "Better Risk Assessment:\n",
        "\n",
        "Ensemble models are more stable, reducing the likelihood of misclassifying high-risk customers.\n",
        "Robustness to Noisy Data:\n",
        "\n",
        "Bagging mitigates the effect of outliers in financial transactions.\n",
        "Interpretability (with feature importance):\n",
        "\n",
        "Even ensemble models like Random Forest provide feature importance scores to identify key predictors of default.\n",
        "Regulatory and Business Confidence:\n",
        "\n",
        "Financial institutions can explain decisions to stakeholders using ensemble-derived insights while maintaining high predictive performance.\n",
        "Step 6 (Optional Advanced Step): Stacking for Maximum Performance\n",
        "Combine multiple ensembles (e.g., Random Forest + XGBoost + Logistic Regression) with a meta-model to capture complementary strengths.\n",
        "Helps improve predictions when different models capture different patterns in transaction history or demographics."
      ],
      "metadata": {
        "id": "rvmjH7z6tTlF"
      }
    }
  ]
}