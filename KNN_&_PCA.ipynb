{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "KGnsBxfku4p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "**Ans.** K-Nearest Neighbors (KNN) is a simple, non-parametric, and instance-based supervised learning algorithm used for both classification and regression tasks. It works by finding the ‘k’ closest data points (neighbors) to a given input and making predictions based on the majority label (for classification) or the average value (for regression) of those neighbors. In classification, KNN assigns the class most common among its k nearest neighbors to the input data point. In regression, KNN calculates the mean (or sometimes the median) of the k nearest neighbors’ output values and assigns this as the predicted value. The distance between data points is usually measured using metrics such as Euclidean distance. KNN does not learn a model in the training phase; instead, it stores the entire training dataset, and predictions are made during the testing phase. The choice of ‘k’ and the distance metric significantly impact the performance of the algorithm."
      ],
      "metadata": {
        "id": "fd9pTCoyvIzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "**Ans.** The Curse of Dimensionality refers to the various problems and challenges that arise when analyzing and organizing data in high-dimensional spaces. As the number of dimensions (features) increases, the volume of the space increases exponentially, and data points become sparse and spread out. This sparsity makes it difficult for machine learning algorithms, especially distance-based models like K-Nearest Neighbors (KNN), to find meaningful patterns.\n",
        "\n",
        "KNN relies heavily on the concept of proximity or similarity between data points, typically using distance metrics such as Euclidean distance. In low-dimensional spaces, the difference in distances between the nearest and farthest neighbors is usually significant, making it easier to distinguish between relevant and irrelevant neighbors. However, in high-dimensional spaces, the distances between data points tend to become similar, making it difficult to identify truly \"nearest\" neighbors. This leads to poor model performance, as the algorithm may include noisy, irrelevant, or distant points in its neighborhood calculations.\n",
        "\n",
        "Moreover, high-dimensional data often includes many irrelevant or redundant features, which can further distort the distance metrics and negatively impact the accuracy of KNN. The increased computational complexity due to more dimensions also slows down the model during both training (data storage) and prediction (distance calculation).\n",
        "\n",
        "To mitigate the effects of the Curse of Dimensionality in KNN, it is essential to apply dimensionality reduction techniques such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), or feature selection methods. These techniques help reduce the number of dimensions by keeping only the most informative features, which improves the reliability of distance measurements and enhances the overall performance of the KNN algorithm."
      ],
      "metadata": {
        "id": "t71nbNxQyMi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "**Ans.** Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features in a dataset while retaining as much of the original variability (information) as possible. It transforms the original correlated features into a new set of uncorrelated variables called principal components. These principal components are ordered so that the first captures the maximum variance in the data, the second captures the next highest variance, and so on. By keeping only the top few principal components, PCA reduces the dimensionality of the data, helping to simplify models, reduce noise, and speed up computation.\n",
        "\n",
        "How PCA Differs from Feature Selection\n",
        "\n",
        "* Feature Extraction vs. Feature Selection: PCA is a feature extraction method that creates new features by combining the original ones into principal components. Feature selection, on the other hand, involves selecting a subset of the existing features without creating new ones.\n",
        "* Interpretability: The principal components created by PCA are linear combinations of the original features and are usually not easy to interpret. Feature selection retains the original features, making the results more interpretable.\n",
        "* Objective: PCA focuses on capturing the maximum variance in the data, regardless of the relevance to the target variable. Feature selection aims to select features that are most relevant and useful for predicting the target.\n",
        "* Methodology: PCA uses mathematical transformations based on eigenvalues and eigenvectors. Feature selection uses statistical tests, correlation analysis, or model-based importance measures to pick relevant features.\n",
        "* Use Cases: PCA is useful when dimensionality reduction is needed but interpretability is less critical. Feature selection is preferred when preserving the meaning of features is important for understanding and explaining the model.\n",
        "\n",
        "For example, if you have 50 features and apply PCA to reduce them to 10 principal components, those 10 components are new features made from combinations of the original 50. In contrast, feature selection would simply keep 10 of the original 50 features based on their importance."
      ],
      "metadata": {
        "id": "WXkMcilD05Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "**Ans.**  Eigenvalues and eigenvectors are fundamental concepts in PCA that help identify the directions in which the data varies the most and quantify the importance of those directions.\n",
        "\n",
        "What Are Eigenvalues and Eigenvectors?\n",
        "\n",
        "* Eigenvectors are vectors that define directions in the feature space along which data variation is measured. In PCA, each eigenvector represents a principal component — a new axis that the data can be projected onto.\n",
        "* Eigenvalues are scalars associated with each eigenvector that indicate the amount of variance (or information) captured along that direction. A higher eigenvalue means the corresponding eigenvector captures more of the data’s variance.\n",
        "\n",
        "Why Are They Important in PCA?\n",
        "\n",
        "* Dimensionality Reduction: PCA uses eigenvectors to find new axes (principal components) that maximize data variance. These axes are the directions where the data spreads out the most.\n",
        "* Variance Quantification: Eigenvalues tell us how much variance each principal component explains. This helps in deciding how many principal components to keep—usually, components with larger eigenvalues are retained because they capture the most important information.\n",
        "* Data Transformation: By projecting the original data onto the eigenvectors, PCA transforms the data into a new coordinate system where features are uncorrelated, making it easier to analyze and model.\n",
        "* Noise Reduction: Components with small eigenvalues often represent noise or less informative parts of the data. Removing these helps in cleaning the dataset and improving model performance.\n",
        "\n",
        "In summary, eigenvectors determine the directions of maximum variance in the data, and eigenvalues quantify how significant each direction is. Together, they enable PCA to effectively reduce dimensionality while preserving essential information."
      ],
      "metadata": {
        "id": "NF64dj3L0_jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "**Ans.** KNN and PCA complement each other well when combined in a single machine learning pipeline, especially when dealing with high-dimensional data.\n",
        "\n",
        "Why Combine PCA and KNN\n",
        "\n",
        "* Dimensionality Reduction: PCA reduces the number of features by transforming the data into a lower-dimensional space while preserving most of the variance. This helps simplify the data and reduces noise, which improves KNN’s effectiveness.\n",
        "* Mitigating the Curse of Dimensionality: KNN relies on distance calculations, which become less meaningful as the number of dimensions increases. By applying PCA first, the feature space is compressed, making distance metrics more reliable and helping KNN find truly nearest neighbors.\n",
        "* Speeding Up Computation: High-dimensional data increases the computational cost of KNN because distances must be calculated across many features. PCA reduces the feature count, which lowers computational complexity and speeds up predictions.\n",
        "* Improving Accuracy: By removing irrelevant or noisy features through PCA, KNN can focus on the most informative components, leading to more accurate classification or regression results.\n",
        "* Simplifying Data Visualization: PCA’s reduced dimensions make it easier to visualize data and understand how KNN classifies or predicts, aiding in model interpretation.\n",
        "\n",
        "In summary, PCA serves as a preprocessing step that enhances KNN’s performance by reducing dimensionality, improving distance calculations, and speeding up computation, making the combined pipeline more efficient and effective.\n",
        "\n",
        "Python example that applies PCA for dimensionality reduction followed by K-Nearest Neighbors (KNN) classification on the Wine dataset from sklearn.\n",
        "\n",
        "    from sklearn.datasets import load_wine\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Load the Wine dataset\n",
        "    data = load_wine()\n",
        "    X, y = data.data, data.target\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Standardize features (important before PCA and KNN)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Apply PCA to reduce dimensionality (e.g., to 2 components for visualization or 5 for balance)\n",
        "    pca = PCA(n_components=5)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    # Initialize KNN classifier (k=3 as an example)\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "    # Train the KNN model on PCA-transformed data\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN classification accuracy after PCA: {accuracy:.4f}\")\n",
        "\n",
        "    # Optional: Explained variance ratio by PCA components\n",
        "    print(\"Explained variance ratio by PCA components:\", pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "tlngeA3a1Nqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8t4Djnpu2Jn",
        "outputId": "a160b125-f03b-4871-dd15-49f5a5719fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling: 0.6852\n",
            "Accuracy with feature scaling: 0.9444\n"
          ]
        }
      ],
      "source": [
        "'''Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# ---- Without Feature Scaling ----\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---- With Feature Scaling ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_with_scaling = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = knn_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy without feature scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with feature scaling: {accuracy_with_scaling:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features before applying PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model (keep all components)\n",
        "pca = PCA(n_components=X.shape[1])\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio for each principal component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, variance_ratio in enumerate(explained_variance, start=1):\n",
        "    print(f\"Principal Component {i}: {variance_ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ajcKy8r7BAl",
        "outputId": "a79f5ae3-f12d-4a70-d3d6-cd70f94c1e12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (important for both PCA and KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ----- KNN on original data -----\n",
        "knn_original = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ----- PCA transformation (top 2 components) -----\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# ----- KNN on PCA-transformed data -----\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy with original features: {accuracy_original:.4f}\")\n",
        "print(f\"Accuracy with top 2 PCA components: {accuracy_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqrgIBSx7KC9",
        "outputId": "3de1f8fb-ddbd-4bd3-b219-450b41303f31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with original features: 0.9444\n",
            "Accuracy with top 2 PCA components: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c4temrw7Xo4",
        "outputId": "d2b792be-9b04-43c0-8b4a-874cb6fea5e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444\n",
            "Accuracy with Manhattan distance: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "(Include your Python code and output\n",
        "\n",
        "**Ans.**\n",
        "\n",
        "✅ Step-by-Step Solution:\n",
        "1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "High-dimensional data can contain redundant or noisy features. PCA transforms the data into a smaller set of uncorrelated components that retain the majority of the variance (information).\n",
        "\n",
        "2. Decide How Many Components to Keep\n",
        "\n",
        "We'll examine the explained variance ratio and choose the smallest number of components that together explain at least 95% of the total variance. This balances dimensionality reduction and information retention.\n",
        "\n",
        "3. Use KNN for Classification\n",
        "\n",
        "Once PCA reduces dimensionality, KNN is used to classify cancer types based on the reduced data. KNN is simple and effective when dimensionality is controlled.\n",
        "\n",
        "4. Evaluate the Model\n",
        "\n",
        "We’ll use accuracy score along with cross-validation to get a reliable estimate of performance, since sample size is small.\n",
        "\n",
        "5. Justify the Pipeline\n",
        "\n",
        "This PCA + KNN pipeline:\n",
        "\n",
        "* Reduces overfitting by eliminating redundant dimensions\n",
        "\n",
        "* Improves generalization by retaining key signals in data\n",
        "\n",
        "* Simplifies the model and makes it computationally efficient\n",
        "\n",
        "* Is a well-established method in biomedical fields for omics data\n",
        "\n",
        "\n",
        "      from sklearn.datasets import make_classification\n",
        "      from sklearn.decomposition import PCA\n",
        "      from sklearn.neighbors import KNeighborsClassifier\n",
        "      from sklearn.model_selection import train_test_split, cross_val_score\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      import matplotlib.pyplot as plt\n",
        "      import numpy as np\n",
        "\n",
        "      # Simulate a gene expression-like dataset: 100 samples, 1000 features\n",
        "      X, y = make_classification(n_samples=100, n_features=1000, n_informative=50, n_classes=3, random_state=42)\n",
        "\n",
        "      # Standardize the dataset\n",
        "      scaler = StandardScaler()\n",
        "      X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "      # Apply PCA (keep all components initially to check explained variance)\n",
        "      pca_full = PCA()\n",
        "      X_pca_full = pca_full.fit_transform(X_scaled)\n",
        "\n",
        "      # Plot cumulative explained variance\n",
        "      cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "      plt.figure(figsize=(8,5))\n",
        "      plt.plot(cumulative_variance, marker='o')\n",
        "      plt.xlabel('Number of Principal Components')\n",
        "      plt.ylabel('Cumulative Explained Variance')\n",
        "      plt.title('Explained Variance vs. Number of Components')\n",
        "      plt.grid(True)\n",
        "      plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      # Decide number of components to retain 95% variance\n",
        "      n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "      print(f\"Number of components to retain 95% variance: {n_components_95}\")\n",
        "\n",
        "      # Apply PCA with optimal number of components\n",
        "      pca = PCA(n_components=n_components_95)\n",
        "      X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "      # Train/Test split\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "      # KNN classifier\n",
        "      knn = KNeighborsClassifier(n_neighbors=3)\n",
        "      knn.fit(X_train, y_train)\n",
        "      accuracy = knn.score(X_test, y_test)\n",
        "      print(f\"Test Accuracy (PCA + KNN): {accuracy:.4f}\")\n",
        "\n",
        "      # Cross-validation for robustness\n",
        "      cv_scores = cross_val_score(knn, X_reduced, y, cv=5)\n",
        "      print(f\"Cross-Validation Accuracy (mean ± std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
      ],
      "metadata": {
        "id": "h8WTazUB_Mj-"
      }
    }
  ]
}