{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCcHWO42lSavRnihuygGj5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Q1.What is Logistic Regression, and how does it differ from Linear Regression?**\n","\n","Ans. **Logistic Regression** is a classification algorithm used to predict the probability of a categorical outcome, usually binary (e.g., 0 or 1). It applies the **sigmoid function** to map predictions to values between 0 and 1.\n","\n","**Difference from Linear Regression:**\n","\n","* Linear Regression predicts **continuous** values.\n","* Logistic Regression predicts **probabilities** for **categorical** outcomes.\n","\n","**Key Differences:**\n","\n","1. **Purpose:**\n","\n","   * Linear: Regression (predict numeric output)\n","   * Logistic: Classification (predict class)\n","\n","2. **Output:**\n","\n","   * Linear: Any real number\n","   * Logistic: Between 0 and 1 (probability)\n","\n","3. **Function Used:**\n","\n","   * Linear: Linear equation\n","   * Logistic: Sigmoid (logistic) function\n","\n","4. **Loss Function:**\n","\n","   * Linear: Mean Squared Error\n","   * Logistic: Cross-Entropy (Log Loss)\n","\n","5. **Decision Boundary:**\n","\n","   * Linear: Not used\n","   * Logistic: Uses threshold (e.g., 0.5) to classify"],"metadata":{"id":"QZKon9aunDvc"}},{"cell_type":"markdown","source":["**Q2. Explain the role of the Sigmoid function in Logistic Regression.**\n","\n","Ans. In Logistic Regression, the **Sigmoid function** is used to convert the output of a linear equation into a **probability value** between **0 and 1**. This probability indicates how likely an input belongs to a particular class, making it suitable for **binary classification**.\n","\n","The sigmoid function is defined as:\n","\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","Where $z$ is the linear combination of input features and model weights.\n","\n","**Key Points:**\n","\n","1. **Probability Output:** Maps any real-valued input to a range between 0 and 1.\n","2. **Classification Decision:** Helps classify data by applying a threshold (e.g., >0.5 = class 1).\n","3. **Smooth Curve:** Provides a smooth gradient, which is useful for optimization using gradient descent.\n","4. **Core Component:** It transforms logistic regression from a linear model to a classifier."],"metadata":{"id":"SMyUT_bAoGAC"}},{"cell_type":"markdown","source":["**Q3. What is Regularization in Logistic Regression and why is it needed?**\n","\n","Ans. **Regularization** is a technique used in Logistic Regression (and other models) to prevent **overfitting** by adding a **penalty** to large model coefficients. It discourages the model from becoming too complex and helps improve its performance on unseen data.\n","\n","**Why Regularization is Needed:**\n","\n","1. **Prevents Overfitting:** Controls the model’s complexity.\n","2. **Improves Generalization:** Helps the model perform better on new data.\n","3. **Stabilizes Coefficients:** Reduces the influence of less important features.\n","4. **Promotes Simplicity:** Encourages simpler, more interpretable models.\n","\n","**Common Types in Logistic Regression:**\n","\n","* **L1 Regularization (Lasso):** Can reduce some coefficients to zero (feature selection).\n","* **L2 Regularization (Ridge):** Shrinks coefficients smoothly but keeps them all."],"metadata":{"id":"Y6y4jm5NpcLf"}},{"cell_type":"markdown","source":["**Q4: What are some common evaluation metrics for classification models, and why are they important?**\n","\n","Evaluation metrics help us measure how well a **classification model** is performing. They provide insights beyond just accuracy, especially when dealing with **imbalanced datasets** or when different types of errors have different costs.\n","\n","\n","**Common Evaluation Metrics:**\n","\n","1. **Accuracy:**\n","\n","   * Proportion of correct predictions.\n","   * Useful when classes are balanced.\n","\n","2. **Precision:**\n","\n","   * Ratio of true positives to predicted positives.\n","   * Important when **false positives** are costly.\n","\n","3. **Recall (Sensitivity):**\n","\n","   * Ratio of true positives to actual positives.\n","   * Important when **false negatives** are critical.\n","\n","4. **F1-Score:**\n","\n","   * Harmonic mean of precision and recall.\n","   * Good balance when both false positives and negatives matter.\n","\n","5. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve):**\n","\n","   * Measures model’s ability to distinguish between classes.\n","   * Useful for comparing classifiers.\n","\n","**Why Important:**\n","\n","* Help choose the best model for a specific problem.\n","* Reveal different types of errors.\n","* Guide model tuning and improvement."],"metadata":{"id":"NMfw6nQ4q32Z"}},{"cell_type":"code","source":["'''Q5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n","splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n","(Use Dataset from sklearn package)'''\n","\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# STEP 1: Load dataset and save to CSV (only needed once)\n","data = load_breast_cancer()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","df.to_csv('breast_cancer_data.csv', index=False)  # Save to CSV\n","\n","# STEP 2: Load CSV into DataFrame\n","df = pd.read_csv('breast_cancer_data.csv')\n","\n","# STEP 3: Split into features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# STEP 4: Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# STEP 5: Train Logistic Regression model\n","model = LogisticRegression(max_iter=10000)\n","model.fit(X_train, y_train)\n","\n","# STEP 6: Predict and evaluate\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAZaTkXstLnS","executionInfo":{"status":"ok","timestamp":1754036414696,"user_tz":-330,"elapsed":6438,"user":{"displayName":"Priyanshu Kumar","userId":"12177200752165252283"}},"outputId":"886007b9-2df8-4c25-9885-50cb547f17e2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.956140350877193\n"]}]},{"cell_type":"code","source":["'''Q6. Write a Python program to train a Logistic Regression model using L2\n","regularization (Ridge) and print the model coefficients and accuracy.'''\n","\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset and save to CSV (optional for assignment)\n","data = load_breast_cancer()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","df.to_csv('breast_cancer_data.csv', index=False)\n","\n","# Read CSV\n","df = pd.read_csv('breast_cancer_data.csv')\n","\n","# Split data\n","X = df.drop('target', axis=1)\n","y = df['target']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression with L2 Regularization\n","model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print results\n","print(\"Model Coefficients:\", model.coef_)\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vE54JjjCuHXx","executionInfo":{"status":"ok","timestamp":1754036558702,"user_tz":-330,"elapsed":650,"user":{"displayName":"Priyanshu Kumar","userId":"12177200752165252283"}},"outputId":"3e982afe-df96-490a-f4e5-41f6a4a9f782"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Coefficients: [[ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n","  -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n","  -2.07613380e-01 -2.97739324e-02 -5.00338038e-02  1.44298427e+00\n","  -3.03857384e-01 -7.25692126e-02 -1.61591524e-02 -1.90655332e-03\n","  -4.48855442e-02 -3.77188737e-02 -4.17516190e-02  5.61347410e-03\n","   1.23214996e+00 -4.04581097e-01 -3.62091502e-02 -2.70867580e-02\n","  -2.62630530e-01 -1.20898539e+00 -1.61796947e+00 -6.15250835e-01\n","  -7.42763610e-01 -1.16960181e-01]]\n","Accuracy: 0.956140350877193\n"]}]},{"cell_type":"code","source":["'''Q7. Write a Python program to train a Logistic Regression model for multiclass\n","classification using multi_class='ovr' and print the classification report.\n","(Use Dataset from sklearn package)'''\n","\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Load dataset and save to CSV (optional)\n","data = load_iris()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","df.to_csv('iris_data.csv', index=False)\n","\n","# Load CSV file\n","df = pd.read_csv('iris_data.csv')\n","\n","# Prepare features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression with one-vs-rest multiclass strategy\n","model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Print classification report\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ug-Qp4VouysR","executionInfo":{"status":"ok","timestamp":1754036792658,"user_tz":-330,"elapsed":13,"user":{"displayName":"Priyanshu Kumar","userId":"12177200752165252283"}},"outputId":"92b6def5-183d-4536-8d1c-0f7824f61105"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00         9\n","           2       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''Q8. Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for\n","Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)'''\n","\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_breast_cancer()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Train/validation split\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a pipeline with scaling and logistic regression\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('clf', LogisticRegression(max_iter=10000))\n","])\n","\n","# Define hyperparameter grid\n","param_grid = {\n","    'clf__penalty': ['l1', 'l2'],\n","    'clf__C': [0.01, 0.1, 1, 10, 100],\n","    'clf__solver': ['liblinear'],  # compatible with l1 and l2\n","}\n","\n","# Setup GridSearchCV\n","grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","grid.fit(X_train, y_train)\n","\n","# Best parameters and validation accuracy\n","best_params = grid.best_params_\n","y_val_pred = grid.predict(X_val)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","\n","print(\"Best Parameters:\", best_params)\n","print(\"Validation Accuracy:\", val_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltkM7SQkvZB0","executionInfo":{"status":"ok","timestamp":1754037020927,"user_tz":-330,"elapsed":4071,"user":{"displayName":"Priyanshu Kumar","userId":"12177200752165252283"}},"outputId":"653089e6-e089-4c8b-cfb2-bfc070bd206d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n","Validation Accuracy: 0.9912280701754386\n"]}]},{"cell_type":"code","source":["'''Q9. Write a Python program to standardize the features before training Logistic Regression and\n","compare the model's accuracy with and without scaling. (Use Dataset from sklearn package)'''\n","\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_breast_cancer()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Prepare features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train/test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# --------- Without Scaling ---------\n","model_no_scaling = LogisticRegression(max_iter=10000, solver='liblinear')\n","model_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = model_no_scaling.predict(X_test)\n","acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","# --------- With Scaling ---------\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","model_with_scaling = LogisticRegression(max_iter=10000, solver='liblinear')\n","model_with_scaling.fit(X_train_scaled, y_train)\n","y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n","acc_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n","\n","# Print comparison\n","print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n","print(f\"Accuracy with scaling:    {acc_with_scaling:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxT1aP4GwcDg","executionInfo":{"status":"ok","timestamp":1754037236932,"user_tz":-330,"elapsed":443,"user":{"displayName":"Priyanshu Kumar","userId":"12177200752165252283"}},"outputId":"38cc2770-3038-4ebc-ad7c-e6040148c3cb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without scaling: 0.9561\n","Accuracy with scaling:    0.9737\n"]}]},{"cell_type":"markdown","source":["Q10. Imagine you are working at an e-commerce company that wants to\n","predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n","\n","Ans. When predicting customer response with a **highly imbalanced dataset** (only 5% responders), the key challenges are handling the imbalance, building a robust model, and ensuring meaningful evaluation.\n","\n","**1. Data Handling**\n","\n","* **Explore and clean data:** Check for missing values, outliers, and inconsistent entries.\n","* **Feature engineering:** Create meaningful features from raw data (e.g., customer demographics, purchase history).\n","* **Feature selection:** Use correlation analysis or feature importance methods to keep relevant features.\n","\n","**2. Feature Scaling**\n","\n","* Scale numerical features using **StandardScaler** (mean=0, std=1) or **MinMaxScaler**.\n","* Scaling helps Logistic Regression converge faster and improves model stability.\n","\n","**3. Handling Class Imbalance**\n","\n","Since only 5% respond, standard training would bias towards predicting non-responders.\n","\n","* **Resampling techniques:**\n","\n","  * **Oversampling** minority class (e.g., using **SMOTE**).\n","  * **Undersampling** majority class.\n","  * Or a combination of both (balanced sampling).\n","* **Use class weights:**\n","\n","  * Set `class_weight='balanced'` in Logistic Regression to penalize mistakes on minority class more heavily.\n","* Prefer **class weights** or **SMOTE** over just accuracy-based methods to preserve data integrity.\n","\n","**4. Model Building & Hyperparameter Tuning**\n","\n","* Use **Logistic Regression** with **L2 regularization** to avoid overfitting.\n","* Tune hyperparameters like:\n","\n","  * **C (inverse regularization strength)**\n","  * **Penalty type (L1/L2)**\n","  * **Solver type**\n","* Employ **GridSearchCV** or **RandomizedSearchCV** with **stratified k-fold cross-validation** to ensure minority class is represented in each fold.\n","* Use **pipeline** to combine scaling and model training cleanly.\n","\n","**5. Evaluation Metrics**\n","\n","* Avoid relying on **accuracy**, which can be misleading due to imbalance.\n","* Focus on:\n","\n","  * **Precision**: Of predicted responders, how many are truly responders?\n","  * **Recall (Sensitivity)**: Of all actual responders, how many did we catch?\n","  * **F1-score**: Harmonic mean of precision and recall for balance.\n","  * **ROC-AUC**: Measures ability to discriminate classes regardless of threshold.\n","  * **Precision-Recall Curve**: More informative when classes are imbalanced.\n","* Consider **business impact** of false positives (wasting marketing budget) vs false negatives (missing potential customers).\n","\n","**6. Final Model & Deployment**\n","\n","* Choose model balancing recall and precision as per business priorities.\n","* Monitor model performance regularly on new data.\n","* Update the model periodically with fresh data to adapt to customer behavior changes."],"metadata":{"id":"ucJ8cShXxlOn"}}]}