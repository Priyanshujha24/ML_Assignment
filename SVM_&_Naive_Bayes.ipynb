{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Support Vector Machine (SVM), and how does it work?**\n",
        "\n",
        "**Ans.** A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a high-dimensional space. The optimal hyperplane is the one that maximizes the margin between the closest points of the classes, which are called support vectors.\n",
        "\n",
        "SVM can handle linear and non-linear classification. For linearly separable data, it finds a straight line (in 2D) or a flat hyperplane (in higher dimensions) that divides the classes. For non-linear data, SVM uses a technique called the kernel trick to transform the input space into a higher-dimensional feature space where a linear separation is possible.\n",
        "\n",
        "Common kernel functions include:\n",
        "\n",
        "* Linear kernel: $K(x, y) = x^T y$\n",
        "* Polynomial kernel: $K(x, y) = (x^T y + c)^d$\n",
        "* Radial Basis Function (RBF) kernel: $K(x, y) = \\exp(-\\gamma \\|x - y\\|^2)$\n",
        "\n",
        "SVM is effective in high-dimensional spaces and is memory efficient because it uses only a subset of training points (support vectors) in the decision function."
      ],
      "metadata": {
        "id": "wHyPF57xWSk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Explain the difference between Hard Margin and Soft Margin SVM.**\n",
        "\n",
        "**Ans.** Hard Margin SVM is used when the data is linearly separable with no misclassification. It tries to find a hyperplane that perfectly separates the data into two classes with the maximum margin and no tolerance for any misclassified points. It assumes that a perfect separation exists, which makes it sensitive to noise and outliers.\n",
        "\n",
        "Soft Margin SVM is used when the data is not perfectly linearly separable. It allows some misclassifications by introducing a penalty for errors in the objective function. It introduces a regularization parameter $C$ that controls the trade-off between maximizing the margin and minimizing classification error. A smaller $C$ allows a wider margin with more tolerance for errors, while a larger $C$ tries to classify all points correctly with a narrower margin.\n",
        "\n",
        "Soft Margin SVM is more robust and practical in real-world scenarios where data may have noise or overlap between classes."
      ],
      "metadata": {
        "id": "t0V1fgZGXV22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.**\n",
        "\n",
        "**Ans.**\n",
        "The Kernel Trick in SVM is a mathematical technique that allows the algorithm to operate in a high-dimensional feature space without explicitly transforming the data. It enables SVM to perform non-linear classification by computing the dot product between the images of the data points in the feature space, using a kernel function. This avoids the computational cost of working in a high-dimensional space directly.\n",
        "\n",
        "One common example is the **Radial Basis Function (RBF) kernel**, also known as the Gaussian kernel, defined as:\n",
        "$K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$\n",
        "where $\\gamma$ is a parameter that defines the spread of the kernel.\n",
        "\n",
        "**Use case**: The RBF kernel is useful when the decision boundary between classes is highly non-linear. For example, in image classification or handwriting recognition, where the data points of different classes are not linearly separable, the RBF kernel helps in mapping the data into a higher-dimensional space where a linear separator can be found."
      ],
      "metadata": {
        "id": "bIWBXubmY0LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is a Naïve Bayes Classifier, and why is it called “naïve”?**\n",
        "\n",
        "**Ans.**\n",
        "A Naïve Bayes Classifier is a probabilistic machine learning model used for classification tasks. It is based on Bayes’ Theorem, which describes the probability of a class given certain features. The formula is:\n",
        "$P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}$\n",
        "where:\n",
        "\n",
        "* $P(C|X)$ is the posterior probability of class $C$ given features $X$,\n",
        "* $P(X|C)$ is the likelihood of features $X$ given class $C$,\n",
        "* $P(C)$ is the prior probability of class $C$,\n",
        "* $P(X)$ is the prior probability of features $X$.\n",
        "\n",
        "The classifier predicts the class with the highest posterior probability for the given input.\n",
        "\n",
        "It is called “naïve” because it **assumes that all features are conditionally independent of each other given the class label**, which is rarely true in real-world scenarios. Despite this unrealistic assumption, the model often performs very well in practice, especially in high-dimensional datasets.\n",
        "\n",
        "**Example**: In email spam detection, the classifier treats the presence of each word in an email as independent from the others, even though words often appear in meaningful combinations. Even with this simplification, Naïve Bayes is highly effective and computationally efficient for classifying spam vs. non-spam emails.\n",
        "\n",
        "Naïve Bayes classifiers are also widely used in sentiment analysis, document classification, and medical diagnosis due to their simplicity, speed, and relatively good performance.\n"
      ],
      "metadata": {
        "id": "TTRdFxX8Y_kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?**\n",
        "\n",
        "**Ans.**\n",
        "Naïve Bayes classifiers have different variants based on the type and distribution of the input features. The three main variants are Gaussian, Multinomial, and Bernoulli Naïve Bayes. Each is suited for different types of data and applications.\n",
        "\n",
        "**1. Gaussian Naïve Bayes:**\n",
        "This variant assumes that the continuous features follow a normal (Gaussian) distribution. It is used when the input data is numerical and continuous in nature. The likelihood of the features is calculated using the Gaussian probability density function.\n",
        "\n",
        "**Use case:**\n",
        "\n",
        "* Used for datasets with continuous features, such as age, salary, or temperature.\n",
        "* Example: Predicting whether a patient has a disease based on blood pressure and cholesterol levels.\n",
        "\n",
        "**2. Multinomial Naïve Bayes:**\n",
        "This variant is used for discrete count data, typically representing the frequency of events (e.g., word counts in documents). It assumes that features follow a multinomial distribution.\n",
        "\n",
        "**Use case:**\n",
        "\n",
        "* Commonly used in text classification tasks where features represent word counts or term frequencies.\n",
        "* Example: Document classification, spam detection, sentiment analysis based on the frequency of words in texts.\n",
        "\n",
        "**3. Bernoulli Naïve Bayes:**\n",
        "This variant is designed for binary/boolean features. It models each feature as being present or absent (1 or 0) and assumes that features follow a Bernoulli distribution.\n",
        "\n",
        "**Use case:**\n",
        "\n",
        "* Suitable for binary feature datasets, such as whether a specific word exists in a document (not how many times it appears).\n",
        "* Example: Email classification using a binary indicator for the presence of certain keywords.\n",
        "\n",
        "**Summary of when to use each:**\n",
        "\n",
        "* **Gaussian NB** → Continuous data (e.g., sensor readings)\n",
        "* **Multinomial NB** → Discrete count data (e.g., word counts in NLP)\n",
        "* **Bernoulli NB** → Binary features (e.g., presence/absence of words or features)"
      ],
      "metadata": {
        "id": "K3zReArEZinz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aeOgZo0WGTQ",
        "outputId": "efa48b67-f821-47bf-f578-1afba1f44843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "'''\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_classifier.support_vectors_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "'''\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred, target_names=cancer.target_names)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AaiflytaT5r",
        "outputId": "f819cbbe-ce10-4f99-e79f-c453e20a5484"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy\n",
        "'''\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Train model using GridSearchCV to find best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set using best estimator\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1exfp4X8al6e",
        "outputId": "503e90b6-90a0-4ab0-e3b4-7db15e33f08c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Hyperparameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Test Set Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Load 20 newsgroups dataset (select two categories for binary classification)\n",
        "categories = ['rec.sport.hockey', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Multinomial Naive Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Since this is binary classification, compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4jb3unka2gd",
        "outputId": "8ec824ad-1d31-4d6f-e4a1-5352ea0436d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. **Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Ans.\n",
        "\n",
        "Preprocessing:\n",
        "\n",
        "Text Vectorization: Use TF-IDF vectorization to convert text emails into numerical feature vectors, which helps capture the importance of words while reducing the impact of common but less informative words.\n",
        "\n",
        "Handling Missing Data: For missing or incomplete emails, either remove them if they are few or fill missing text with placeholders (like empty strings) so vectorization can still be applied without errors.\n",
        "\n",
        "Text Cleaning: Lowercasing, removing punctuation, stopwords, and stemming/lemmatization can improve feature quality.\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "Naïve Bayes is often preferred for spam classification due to its efficiency, ability to handle high-dimensional sparse data, and strong performance on text data with diverse vocabulary. It handles word independence assumptions well despite real-world correlations.\n",
        "\n",
        "SVM can also be used and might perform better with fine-tuned kernels but is computationally heavier and slower on large datasets.\n",
        "\n",
        "For a real-time or large-scale system, Naïve Bayes is typically preferred.\n",
        "\n",
        "Addressing Class Imbalance:\n",
        "\n",
        "Use techniques such as class weighting in models or resampling methods (like SMOTE for oversampling spam emails or downsampling majority class).\n",
        "\n",
        "Alternatively, use threshold tuning based on precision-recall trade-offs to favor detecting spam while minimizing false positives.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Accuracy alone is misleading with class imbalance. Use Precision, Recall, and F1-score to balance false positives and false negatives.\n",
        "\n",
        "ROC-AUC and Precision-Recall AUC are useful to evaluate classifier performance across thresholds.\n",
        "\n",
        "In spam detection, high recall (catching most spam) and high precision (not labeling legitimate emails as spam) are critical.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Accurate spam filtering reduces user frustration and security risks (e.g., phishing).\n",
        "\n",
        "Minimizing false positives preserves customer trust by avoiding loss of legitimate emails.\n",
        "\n",
        "Efficient classification saves resources by reducing manual email sorting."
      ],
      "metadata": {
        "id": "nw8F7WKWbkaG"
      }
    }
  ]
}